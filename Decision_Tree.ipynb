{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree using Entropy and Information Gain\n",
    "Ryan Miller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in and Splitting the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading in the Pima Indians Diabetes dataset\n",
    "data = pd.read_csv('../Data/diabetes.csv')\n",
    "\n",
    "#splitting data into features and target variables\n",
    "target = np.array(data.iloc[:,-1]).reshape((-1,1))\n",
    "features = data.iloc[:,:-1]\n",
    "\n",
    "#scaling the features to mean 0 and unit variance\n",
    "ss = StandardScaler()\n",
    "features = ss.fit_transform(np.array(features))\n",
    "\n",
    "#adding intercept column to features\n",
    "features = np.append(features,np.ones((features.shape[0],1)),axis=1)\n",
    "\n",
    "#splitting the data into train and test sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Information Gain for Deciding on Decision Tree Split Values\n",
    "Information Gain = Entropy before Split - Entropy after Split  \n",
    "Entropy = $ \\sum^c_{i=1} -p_i*log_2(p_i) $  \n",
    "where $c$ is the number of classes and $p_i$ is the probability that an observation belongs to the current class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computes entropy for information gain\n",
    "def entropy(class_y):\n",
    "    unique_vals = list(np.unique(class_y))\n",
    "    entropy = 0\n",
    "    for val in unique_vals:\n",
    "        prob = sum([1 for i in class_y if i == val])/len(class_y)\n",
    "        entropy += prob*np.log2(prob)\n",
    "    return -entropy\n",
    "\n",
    "#partitioning classes based on given split value and variable\n",
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    X_left = []\n",
    "    X_right = []\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "\n",
    "    #splitting data when split_val is categorical\n",
    "    if type(split_val) is str:\n",
    "        for i in range(len(X)):\n",
    "            if X[i][split_attribute] == split_val:\n",
    "                X_left.append(X[i])\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(X[i])\n",
    "                y_right.append(y[i])\n",
    "                \n",
    "    #splitting data when split_val is continuous\n",
    "    else:\n",
    "        for i in range(len(X)):\n",
    "            if X[i][split_attribute] <= split_val:\n",
    "                X_left.append(X[i])\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(X[i])\n",
    "                y_right.append(y[i])\n",
    "\n",
    "    return (X_left, X_right, y_left, y_right)\n",
    "\n",
    "#computing information gained from the current split\n",
    "def information_gain(previous_y, current_y):\n",
    "    #calculating previous entropy\n",
    "    entropy_prev = entropy(previous_y)\n",
    "\n",
    "    #calculating current entropy given split\n",
    "    entropy_cur = 0\n",
    "    total_len = sum(len(i) for i in current_y)\n",
    "    for i in current_y:\n",
    "        entropy_cur += entropy(i)*(len(i)/total_len)\n",
    "\n",
    "    return entropy_prev - entropy_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self,depth=0):\n",
    "        # Initializing the tree as a dictionary with a depth of 0\n",
    "        self.tree = {'depth':depth}\n",
    "        pass\n",
    "    \n",
    "    #recursively training the decision tree using the given data\n",
    "    def learn(self, X, y, max_depth=15):\n",
    "        X_arr = np.array(X)\n",
    "        #checking stopping conditions (y only has one class or depth is >= given max_depth)\n",
    "        if len(np.unique(y)) == 1 or self.tree['depth'] >= max_depth:\n",
    "            self.tree['is_leaf'] = True\n",
    "            self.tree['class'] = stats.mode(y).mode\n",
    "        else:\n",
    "            self.tree['is_leaf'] = False\n",
    "            #for each attribute, finding split value that gives max information gain\n",
    "            info_gain_list = []\n",
    "            split_val_list = []\n",
    "            \n",
    "            #looping over all columns of X\n",
    "            for i in range(X_arr.shape[1]):\n",
    "                #using mode of categorical values for split\n",
    "                if type(X_arr[0,i]) is str:\n",
    "                    split_val = stats.mode(X_arr[:,i]).mode[0]\n",
    "                #using mean of continuous variables for split\n",
    "                else:\n",
    "                    split_val = np.mean(X_arr[:,i])\n",
    "                #splitting X based on split val and current attribute\n",
    "                (X_left, X_right, y_left, y_right) = partition_classes(X, y, i, split_val)\n",
    "                #storing current information gain and split val\n",
    "                info_gain_list.append(information_gain(y,[y_left,y_right]))\n",
    "                split_val_list.append(split_val)\n",
    "\n",
    "            #selecting split attribute that gives max information gain\n",
    "            split_attribute = np.argmax(info_gain_list)\n",
    "            split_val = split_val_list[split_attribute]\n",
    "\n",
    "            #splitting data based on best split attribute\n",
    "            (X_left, X_right, y_left, y_right) = partition_classes(X, y, split_attribute, split_val)\n",
    "            \n",
    "            #storing split attribute and value\n",
    "            self.tree['split_attribute'] = split_attribute\n",
    "            self.tree['split_val'] = split_val_list[split_attribute]\n",
    "            \n",
    "            #recursively training child nodes\n",
    "            self.tree['left'] = DecisionTree(depth=self.tree['depth']+1)\n",
    "            self.tree['left'].learn(X_left,y_left,max_depth)\n",
    "            self.tree['right'] = DecisionTree(depth=self.tree['depth']+1)\n",
    "            self.tree['right'].learn(X_right,y_right,max_depth)\n",
    "            \n",
    "    #classifying one observation using trained tree\n",
    "    def classify_one(self, record):\n",
    "        if self.tree['is_leaf'] == True:\n",
    "            return self.tree['class'][0]\n",
    "        else:\n",
    "            #splitting by categorical split_val\n",
    "            if type(self.tree['split_val']) is str:\n",
    "                #deciding split direction\n",
    "                if record[self.tree['split_attribute']] == self.tree['split_val']:\n",
    "                    return self.tree['left'].classify_one(record)\n",
    "                else:\n",
    "                    return self.tree['right'].classify_one(record)\n",
    "            #splitting by continuous split_val\n",
    "            else:\n",
    "                #deciding split direction\n",
    "                if record[self.tree['split_attribute']] <= self.tree['split_val']:\n",
    "                    return self.tree['left'].classify_one(record)\n",
    "                else:\n",
    "                    return self.tree['right'].classify_one(record)\n",
    "                \n",
    "    #classifying a set of observations\n",
    "    def classify(self,records):\n",
    "        return [self.classify_one(i)[0] for i in records] \n",
    "    \n",
    "    #scoring the decision tree based on given test features and labels\n",
    "    def score(self,X,y):\n",
    "        y_pred = np.array(self.classify(X)).reshape((-1))\n",
    "        return np.mean(y_pred==y.reshape((-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Performance to Sklearn's DecisionTreeClassifier\n",
    "The test accuracy of my remade Decision Tree Classifier is comparable to Sklearn's implementation, and unlike Sklearn's version, it is capable of handling categorical data without requiring preprocessing beforehand. The main downside that comes to mind is the noticeably slower runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn's DecisionTreeClassifier Test Accuracy: 72.08 %\n",
      "Sklearn's DecisionTreeClassifier Runtime: 0.003964 seconds\n"
     ]
    }
   ],
   "source": [
    "#sklearn\n",
    "start = time()\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\",random_state=0,max_depth=25)\n",
    "dtc.fit(X_train,y_train)\n",
    "end = time()\n",
    "print(\"Sklearn's DecisionTreeClassifier Test Accuracy:\",np.round(100*dtc.score(X_test,y_test),2),'%')\n",
    "print(\"Sklearn's DecisionTreeClassifier Runtime:\",np.round(end-start,6),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Made DecisionTree Classifier Test Accuracy: 75.32 %\n",
      "Self-Made DecisionTree Classifier Runtime: 0.390928 seconds\n"
     ]
    }
   ],
   "source": [
    "#self-made\n",
    "start = time()\n",
    "dt = DecisionTree()\n",
    "dt.learn(X_train,y_train,max_depth=25)\n",
    "end = time()\n",
    "print(\"Self-Made DecisionTree Classifier Test Accuracy:\",np.round(100*dt.score(X_test,y_test),2),'%')\n",
    "print(\"Self-Made DecisionTree Classifier Runtime:\",np.round(end-start,6),'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
