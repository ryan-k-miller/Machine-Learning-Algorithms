{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using Entropy and Information Gain\n",
    "Ryan Miller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DecisionTree'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-332e101c7b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mDecisionTree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpartition_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'DecisionTree'"
     ]
    }
   ],
   "source": [
    "from DecisionTree import entropy,partition_classes,information_gain,DecisionTree\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in and Splitting the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading in the Pima Indians Diabetes dataset\n",
    "data = pd.read_csv('../Data/diabetes.csv')\n",
    "\n",
    "#splitting data into features and target variables\n",
    "target = np.array(data.iloc[:,-1]).reshape((-1,1))\n",
    "features = data.iloc[:,:-1]\n",
    "\n",
    "#scaling the features to mean 0 and unit variance\n",
    "ss = StandardScaler()\n",
    "features = ss.fit_transform(np.array(features))\n",
    "\n",
    "#adding intercept column to features\n",
    "features = np.append(features,np.ones((features.shape[0],1)),axis=1)\n",
    "\n",
    "#splitting the data into train and test sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    #list to contain the decision trees made during fitting\n",
    "    decision_trees = []\n",
    "\n",
    "    #the bootstrapping datasets for trees\n",
    "    #bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    #the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    #bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in \n",
    "    #the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def _bootstrapping(self, X, y, boot_size):      \n",
    "        #finding random sample of indices\n",
    "        np.random.seed = 0\n",
    "        idxs = np.random.randint(low=0,high=len(X),size=int(boot_size*len(X)))\n",
    "        #creating the bootstrapped features and labels\n",
    "        samples = [list(X[i,:]) for i in idxs]\n",
    "        labels = [y[i] for i in idxs]\n",
    "        return (samples, labels)\n",
    "\n",
    "    def bootstrapping(self, X, y, boot_size):\n",
    "        #checking to see if bootstraps_datasets is already populated\n",
    "        if len(self.bootstraps_datasets) > 0:\n",
    "            return\n",
    "        #initializing one bootstapped dataset for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(X, y, boot_size)\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self, X, y, num_trees = 20, max_depth = 15, boot_size = 0.20):\n",
    "        #initializing decision trees\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree() for i in range(num_trees)]\n",
    "        #self.bootstraps_datasets = []\n",
    "        #creating bootstrapped datasets\n",
    "        self.bootstrapping(X,y,boot_size)\n",
    "        #training the decision trees using the bootstrapped datasets\n",
    "        for i in range(len(self.decision_trees)):\n",
    "            self.decision_trees[i].learn(X=self.bootstraps_datasets[i],y=self.bootstraps_labels[i], max_depth=max_depth)\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = []\n",
    "        #looping over all observations in X\n",
    "        for record in X:\n",
    "            votes = []\n",
    "            #looping over all bootstrapped datasets\n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                #if the record is not in the bootstrapped dataset\n",
    "                #getting the votes from the out-of-bag trees\n",
    "                if list(record) not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify_one(record)\n",
    "                    votes.append(effective_vote[0])\n",
    "            counts = np.bincount(votes)\n",
    "            \n",
    "            #if the record is not an out-of-bag sample for any of the trees\n",
    "            #take the majority vote of all the trees \n",
    "            if len(counts) == 0:\n",
    "                for i in range(len(self.bootstraps_datasets)):\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify_one(record)\n",
    "                    votes.append(effective_vote)\n",
    "                counts = np.bincount(votes)\n",
    "                y = np.append(y, np.argmax(counts))             \n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Performance to Sklearn's RandomForestClassifier\n",
    "The test accuracy of my remade Random Forest Classifier is comparable to Sklearn's implementation, and unlike Sklearn's version, it is capable of handling categorical data without requiring preprocessing beforehand. The main downside is the noticeably slower runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn's DecisionTreeClassifier Test Accuracy: 73.698 %\n",
      "Sklearn's DecisionTreeClassifier Runtime: 0.057855 seconds\n"
     ]
    }
   ],
   "source": [
    "#sklearn\n",
    "start = time()\n",
    "rf = RandomForestClassifier(n_estimators = 20,criterion=\"entropy\",random_state=0,max_depth=15,oob_score=True)\n",
    "rf.fit(features,np.ravel(target))\n",
    "end = time()\n",
    "print(\"Sklearn's DecisionTreeClassifier Test Accuracy:\",np.round(100*rf.oob_score_,3),'%')\n",
    "print(\"Sklearn's DecisionTreeClassifier Runtime:\",np.round(end-start,6),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Made Decision Tree Classifier Test Accuracy: 75.391 %\n",
      "Self-Made Decision Tree Classifier Runtime: 1.590771 seconds\n"
     ]
    }
   ],
   "source": [
    "#self-made\n",
    "start = time()\n",
    "randomForest = RandomForest()\n",
    "randomForest.fitting(features, target, max_depth=15, num_trees=20)\n",
    "y_predicted = randomForest.voting(features)\n",
    "end = time()\n",
    "print(\"Self-Made Decision Tree Classifier Test Accuracy:\",np.round(100*np.mean(y_predicted == np.ravel(target)),3),'%')\n",
    "print(\"Self-Made Decision Tree Classifier Runtime:\",np.round(end-start,6),'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
